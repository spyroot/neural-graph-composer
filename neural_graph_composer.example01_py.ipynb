{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Neural Graph Composer Inductive settings\n",
    "GCN GCN/GAT/GIN Setting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  %cd /root\n",
    "  !rm -rf /root/neural-graph-composer\n",
    "  !git clone https://github.com/spyroot/neural-graph-composer\n",
    "  %cd neural-graph-composer\n",
    "  !ls\n",
    "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu118.html\n",
    "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu118.html\n",
    "  !pip install torch-geometric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch_geometric\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_geometric.nn import GCNConv, GINConv, GATConv\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from example_shared import Experiments\n",
    "from neural_graph_composer.midi_dataset import MidiDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standard GCN 2 layers.\n",
    "A standard Graph Convolutional Network (GCN) layer is a building block used in graph neural networks.\n",
    "It takes as input a graph represented as an adjacency matrix and a feature matrix for each node\n",
    "in the graph. The layer performs a linear transformation of the feature matrix,\n",
    "which is then aggregated with the features of neighboring nodes in the graph. This aggregation\n",
    "step is done by taking a weighted sum of the features of neighboring nodes, where the weights\n",
    "are determined by the adjacency matrix. The resulting feature matrix is then passed through an\n",
    "activation function, such as ReLU, to produce the output of the layer.\n",
    "\n",
    "The weights in the linear transformation and the aggregation step are learned through\n",
    "backpropagation during training. The number of nodes in the graph can vary across\n",
    "different graphs, so standard GCN layers use weight sharing to ensure that the same\n",
    "set of weights is used for all nodes in the graph. This makes the layer computationally\n",
    "efficient and scalable to large graphs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GCN2(torch.nn.Module):\n",
    "    def __init__(self, num_feature, hidden_channels, num_classes):\n",
    "        super(GCN2, self).__init__()\n",
    "        self.conv1 = GCNConv(num_feature, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "HEre we create 3 Layer GCN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class GCN3(torch.nn.Module):\n",
    "    def __init__(self, num_feature, hidden_channels, num_classes):\n",
    "        super(GCN3, self).__init__()\n",
    "        self.conv1 = GCNConv(num_feature, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GIN is a generalization of the GCN (Graph Convolutional Network) architecture, which was designed specifically for nodeclassification tasks on homogeneous graphs. The key idea behind GIN is to use\n",
    "an MLP (multilayer perceptron) to learn a node embedding, which is then aggregated\n",
    "and normalized to produce the final node embedding. In contrast to GCN, which uses a\n",
    "linear transformation followed by a nonlinearity to update node embeddings,\n",
    "GIN applies a nonlinearity (ReLU) immediately after the MLP, followed by a\n",
    "mean aggregator and a learnable bias term."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, num_feature, hidden_channels, num_classes):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_feature, hidden_channels), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels)))\n",
    "        self.conv2 = GINConv(torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels)))\n",
    "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature, hidden_channels, num_classes,\n",
    "                 use_edge_weights=True, dropout=0.3):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.use_edge_weights = use_edge_weights\n",
    "        self.conv1 = GATConv(num_feature, hidden_channels, add_self_loops=True)\n",
    "        self.conv2 = GATConv(hidden_channels, num_classes, add_self_loops=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if self.use_edge_weights:\n",
    "            x = self.conv1(x, edge_index, edge_weight)\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if self.use_edge_weights:\n",
    "            x = self.conv2(x, edge_index, edge_weight)\n",
    "        else:\n",
    "            x = self.conv2(x, edge_index)\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Experiments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mExample01\u001B[39;00m(\u001B[43mExperiments\u001B[49m):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m      3\u001B[0m             \u001B[38;5;28mself\u001B[39m, epochs: \u001B[38;5;28mint\u001B[39m, batch_size: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m      4\u001B[0m             midi_dataset: MidiDataset, hidden_dim: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m      5\u001B[0m             model_type: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGCN3\u001B[39m\u001B[38;5;124m\"\u001B[39m, lr: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.01\u001B[39m):\n\u001B[1;32m      6\u001B[0m \u001B[38;5;250m        \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m           Example experiment for training a graph neural network on MIDI data.\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m        :param epochs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m        :param lr:\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m        \"\"\"\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Experiments' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class Example01(Experiments):\n",
    "    def __init__(\n",
    "            self, epochs: int, batch_size: int,\n",
    "            midi_dataset: MidiDataset, hidden_dim: int,\n",
    "            model_type: Optional[str] = \"GCN3\", lr: Optional[float] = 0.01):\n",
    "        \"\"\"\n",
    "           Example experiment for training a graph neural network on MIDI data.\n",
    "        :param epochs:\n",
    "        :param batch_size:\n",
    "        :param hidden_dim:\n",
    "        :param model_type:\n",
    "        :param lr:\n",
    "        \"\"\"\n",
    "        super().__init__(epochs, batch_size, midi_dataset)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        assert self.device is not None, \"Device is not set.\"\n",
    "        assert self.datasize is not None, \"Datasize is not set.\"\n",
    "        assert self.test_size is not None, \"Test size is not set.\"\n",
    "        assert self._num_workers is not None, \"Number of workers is not set.\"\n",
    "        assert self._batch_size is not None, \"Batch size is not set.\"\n",
    "\n",
    "        self.datasize = 0\n",
    "        self.test_size = 0\n",
    "        self._num_workers = 0\n",
    "        self._batch_size = batch_size\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._feature_dim = midi_dataset.num_node_features\n",
    "        self._num_classes = midi_dataset.total_num_classes\n",
    "        self._is_gin = False\n",
    "        self._is_gat = False\n",
    "        self._lr = lr\n",
    "\n",
    "        self.train_dataset = midi_dataset\n",
    "        self.test_dataset = midi_dataset\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            midi_dataset, batch_size=self._batch_size, shuffle=True)\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            midi_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            midi_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        if model_type == \"GCN3\":\n",
    "            print(f\"Creating GCN3 {self._feature_dim} {self._hidden_dim} {self._num_classes}\")\n",
    "            self.model = GCN3(\n",
    "                self._feature_dim, self._hidden_dim, self._num_classes)\n",
    "        elif model_type == \"GAT\":\n",
    "            print(f\"Creating GAT {self._feature_dim} {self._hidden_dim} {self._num_classes}\")\n",
    "            self.model = GAT(\n",
    "                self._feature_dim, self._hidden_dim, self._num_classes)\n",
    "            self._is_gat = True\n",
    "        else:\n",
    "            self.model = GIN(\n",
    "                self._feature_dim, self._hidden_dim, self._num_classes)\n",
    "            self._is_gin = True\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self._lr, weight_decay=5e-4)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        loss_all = 0.0\n",
    "        tp = 0.0\n",
    "        fp = 0.0\n",
    "        fn = 0.0\n",
    "\n",
    "        pred_train_all = []\n",
    "        y_train_all = []\n",
    "\n",
    "        for i, b in enumerate(self.train_loader):\n",
    "            train_batch, _, _ = b\n",
    "            train_batch.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(train_batch)\n",
    "\n",
    "            train_mask = train_batch.train_mask\n",
    "            y_train = train_batch.y[train_mask]\n",
    "\n",
    "            if self._is_gin:\n",
    "                node_idx = torch.arange(out.shape[0]).to(self.device)\n",
    "                out_sum = torch.zeros((train_batch.num_nodes, out.shape[1]), dtype=torch.float).to(self.device)\n",
    "                out_sum.scatter_(0, node_idx.unsqueeze(1).expand(-1, out.shape[1]), out)\n",
    "                out_train = out_sum[train_mask]\n",
    "            elif self._is_gat:\n",
    "                node_idx = torch.arange(out.shape[0]).to(self.device)\n",
    "                out_sum = torch.zeros((train_batch.num_nodes, out.shape[1]), dtype=torch.float).to(self.device)\n",
    "                out_sum.scatter_(0, node_idx.unsqueeze(1).expand(-1, out.shape[1]), out)\n",
    "                out_train = out_sum[train_mask, -self._num_classes:]\n",
    "            else:\n",
    "                out_train = out[train_mask]\n",
    "\n",
    "            loss = F.nll_loss(out_train, y_train)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss_all += train_batch.num_graphs * loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate F1 and accuracy metrics\n",
    "            pred_train = out_train.argmax(dim=1).cpu().numpy()\n",
    "            y_train_np = y_train.cpu().numpy()\n",
    "            pred_train_all.append(pred_train)\n",
    "            y_train_all.append(y_train_np)\n",
    "\n",
    "            pred_class_idx = torch.argmax(out_train, dim=1)\n",
    "            tp += torch.sum((pred_class_idx == 1) & (y_train == 1)).item()\n",
    "            fp += torch.sum((pred_class_idx == 1) & (y_train == 0)).item()\n",
    "            fn += torch.sum((pred_class_idx == 0) & (y_train == 1)).item()\n",
    "\n",
    "        # Calculate F1 and accuracy metrics\n",
    "        pred_train_all = np.concatenate(pred_train_all, axis=0)\n",
    "        y_train_all = np.concatenate(y_train_all, axis=0)\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        train_f1 = f1_score(y_train_all, pred_train_all, average='macro')\n",
    "        train_acc = (pred_train_all == y_train_all).sum() / len(y_train_all)\n",
    "\n",
    "        loss_avg = loss_all / len(self.train_loader.dataset)\n",
    "        return loss_avg, train_f1, train_acc, recall, precision\n",
    "    @staticmethod\n",
    "    def plot_metrics(\n",
    "            self, train_loss, train_f1, train_acc,\n",
    "            train_precision, train_recall,\n",
    "            val_acc, val_f1, val_precision, val_recall,\n",
    "            test_acc, test_f1, test_precision, test_recall,\n",
    "            num_epochs, output_dir=\"metric\", run=None):\n",
    "        \"\"\"Plot and save to a file\n",
    "        \"\"\"\n",
    "        epochs = range(1, num_epochs + 1)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.plot(epochs, train_loss, label=\"train_loss\")\n",
    "        plt.plot(epochs, train_f1, label=\"train_f1\")\n",
    "        plt.plot(epochs, train_acc, label=\"train_acc\")\n",
    "        plt.plot(epochs, train_precision, label=\"train_precision\")\n",
    "        plt.plot(epochs, train_recall, label=\"train_recall\")\n",
    "        plt.plot(epochs, val_acc, label=\"val_acc\")\n",
    "        plt.plot(epochs, val_f1, label=\"val_f1\")\n",
    "        plt.plot(epochs, val_precision, label=\"val_precision\")\n",
    "        plt.plot(epochs, val_recall, label=\"val_recall\")\n",
    "        plt.plot(epochs, test_acc, label=\"test_acc\")\n",
    "        plt.plot(epochs, test_f1, label=\"test_f1\")\n",
    "        plt.plot(epochs, test_precision, label=\"test_precision\")\n",
    "        plt.plot(epochs, test_recall, label=\"test_recall\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Metric\")\n",
    "        plt.title(\"Training Metrics\")\n",
    "        plt.legend()\n",
    "\n",
    "        if output_dir:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            filename = os.path.join(output_dir, \"metrics.png\")\n",
    "            plt.savefig(filename)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        best_val_acc = 0.\n",
    "        best_epoch = 0.\n",
    "        best_test_acc = 0.\n",
    "\n",
    "        train_losses = []\n",
    "        train_f1s = []\n",
    "        train_accs = []\n",
    "        train_recalls = []\n",
    "        train_precisions = []\n",
    "\n",
    "        val_accs = []\n",
    "        val_f1s = []\n",
    "        val_precisions = []\n",
    "        val_recalls = []\n",
    "\n",
    "        test_accs = []\n",
    "        test_f1s = []\n",
    "        test_precisions = []\n",
    "        test_recalls = []\n",
    "\n",
    "        for e in range(1, self._epochs + 1):\n",
    "\n",
    "            loss_avg, train_f1, train_acc, train_recall, train_precision = self.train_epoch()\n",
    "            # train_acc, train_f1, train_precision, train_recall = self.evaluate(self.train_loader)\n",
    "            val_acc, val_f1, val_precision, val_recall = self.evaluate(self.val_loader)\n",
    "            test_acc, test_f1, test_precision, test_recall = self.evaluate(self.test_loader)\n",
    "\n",
    "            train_losses.append(loss_avg)\n",
    "            train_f1s.append(train_f1)\n",
    "            train_accs.append(train_acc)\n",
    "            train_recalls.append(train_recall)\n",
    "            train_precisions.append(train_precision)\n",
    "\n",
    "            val_accs.append(val_acc)\n",
    "            val_f1s.append(val_f1)\n",
    "            val_precisions.append(val_precision)\n",
    "            val_recalls.append(val_recall)\n",
    "\n",
    "            test_accs.append(test_acc)\n",
    "            test_f1s.append(test_f1)\n",
    "            test_precisions.append(test_precision)\n",
    "            test_recalls.append(test_recall)\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = e\n",
    "                best_test_acc = test_acc\n",
    "\n",
    "            if e % 5 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {e}, \"\n",
    "                    f\"Loss: {loss_avg:.5f}, \"\n",
    "                    f\"Train Acc: {train_acc:.5f}, \"\n",
    "                    f\"Train F1: {train_f1:.5f}, \"\n",
    "                    f\"Train Precision: {train_precision:.5f}, \"\n",
    "                    f\"Train Recall: {train_recall:.5f}\")\n",
    "\n",
    "            if e % 10 == 0:\n",
    "                val_acc, val_f1, val_precision, val_recall = self.evaluate(\"val\")\n",
    "                test_acc, test_f1, test_precision, test_recall = self.evaluate(\"test\")\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch: {e}, \"\n",
    "                    f\"Val Acc: {val_acc:.5f}, \"\n",
    "                    f\"Val F1: {val_f1:.5f}, \"\n",
    "                    f\"Val Precision: {val_precision:.5f}, \"\n",
    "                    f\"Val Recall: {val_recall:.5f}, \"\n",
    "                    f\"Test Acc: {test_acc:.5f}, \"\n",
    "                    f\"Test F1: {test_f1:.5f}, \"\n",
    "                    f\"Test Precision: {test_precision:.5f}, \"\n",
    "                    f\"Test Recall: {test_recall:.5f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = e\n",
    "                best_test_acc = test_acc\n",
    "\n",
    "        print(f\"Best Epoch: {best_epoch}, Test Acc: {best_test_acc:.5f}\")\n",
    "        self.plot_metrics(\n",
    "            train_losses, train_f1s, train_accs, train_precisions,\n",
    "            train_recalls, val_accs, val_f1s, val_precisions, val_recalls,\n",
    "            test_accs, test_f1s, test_precisions, test_recalls, self._epochs )\n",
    "\n",
    "    def evaluate(self, eval_type: str):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if eval_type == \"val\":\n",
    "            data_loader = self.val_loader\n",
    "        else:\n",
    "            data_loader = self.test_loader\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for b in data_loader:\n",
    "            if eval_type == \"val\":\n",
    "                _, val, _ = b\n",
    "                batch = val\n",
    "            else:\n",
    "                _, _, test_b = b\n",
    "                batch = test_b\n",
    "\n",
    "            test_mask = batch.test_mask\n",
    "            data = batch.to(self.device)\n",
    "            out = self.model(data)\n",
    "\n",
    "            if self._is_gin:\n",
    "                node_idx = torch.arange(out.shape[0]).to(self.device)\n",
    "                out_sum = torch.zeros((batch.num_nodes, out.shape[1]), dtype=torch.float).to(self.device)\n",
    "                out_sum.scatter_(0, node_idx.unsqueeze(1).expand(-1, out.shape[1]), out)\n",
    "                pred_masked = out_sum[test_mask]\n",
    "            elif self._is_gat:\n",
    "                node_idx = torch.arange(out.shape[0]).to(self.device)\n",
    "                out_sum = torch.zeros((batch.num_nodes, out.shape[1]), dtype=torch.float).to(self.device)\n",
    "                out_sum.scatter_(0, node_idx.unsqueeze(1).expand(-1, out.shape[1]), out)\n",
    "                pred_masked = out_sum[test_mask, -self._num_classes:]\n",
    "            else:\n",
    "                pred_masked = out[test_mask]\n",
    "\n",
    "            y_masked = batch.y[test_mask]\n",
    "            pred_class_idx = torch.argmax(pred_masked, dim=1)\n",
    "            correct += torch.sum(torch.eq(pred_class_idx, y_masked)).item()\n",
    "\n",
    "            _y_masked = batch.y[test_mask].cpu().numpy()\n",
    "            _pred_class_idx = torch.argmax(pred_masked, dim=1).cpu().numpy()\n",
    "            _accuracy = accuracy_score(_y_masked, _pred_class_idx)\n",
    "\n",
    "            # calculate tp, fp, fn for F1 score, precision, and recall\n",
    "            tp += torch.sum((pred_class_idx == 1) & (y_masked == 1)).item()\n",
    "            fp += torch.sum((pred_class_idx == 1) & (y_masked == 0)).item()\n",
    "            fn += torch.sum((pred_class_idx == 0) & (y_masked == 1)).item()\n",
    "\n",
    "            total += y_masked.shape[0]\n",
    "\n",
    "        accuracy = correct / total\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "\n",
    "        return accuracy, f1, precision, recall\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Mode training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--epochs', type=int, default=10)\n",
    "parser.add_argument('--model_type', type=str, default='GCN3', choices=['GCN3', 'GIN', 'GAT'])\n",
    "parser.add_argument('--hidden_dim', type=int, default=32)\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--graph_per_instrument', type=bool, default=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.1,\n",
    "        is_undirected=True,\n",
    "        split_labels=True,\n",
    "        add_negative_train_samples=False)\n",
    "])\n",
    "\n",
    "ds = MidiDataset(root=\"./data\", transform=transform, per_instrument_graph=args.graph_per_instrument)\n",
    "\n",
    "example = Example01(\n",
    "    epochs=args.epochs,\n",
    "    batch_size=args.batch_size,\n",
    "    midi_dataset=ds,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    model_type=args.model_type,\n",
    "    lr=args.lr)\n",
    "example.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
