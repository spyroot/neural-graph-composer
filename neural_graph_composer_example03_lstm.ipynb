{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  %cd /root\n",
    "  !rm -rf /root/neural-graph-composer\n",
    "  !git clone https://github.com/spyroot/neural-graph-composer\n",
    "  %cd neural-graph-composer\n",
    "  !ls\n",
    "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu118.html\n",
    "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu118.html\n",
    "  !pip install torch-geometric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "THis LSTM and GCN Model.   GCN Used for downstream task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import heapq\n",
    "from typing import Optional\n",
    "\n",
    "import argparse\n",
    "import heapq\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from example_shared import Experiments\n",
    "from neural_graph_composer.midi_dataset import MidiDataset\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Two-layer Graph Convolutional Network (GCN) that maps node features to node embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, hidden_channels: int):\n",
    "        \"\"\"\n",
    "        :param num_features: (int): The number of input features for each node.\n",
    "        :param hidden_channels: (int): The number of output channels in the GCN layer.\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN3(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Three-layer Graph Convolutional Network (GCN) that maps node features to class predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature: int,\n",
    "                 hidden_channels: int,\n",
    "                 num_classes: int,\n",
    "                 dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        :param num_feature: Number of input features per node.\n",
    "        :param hidden_channels: Size of the output feature space\n",
    "                                of the first and second convolutional layers.\n",
    "        :param num_classes: Number of classes to predict.\n",
    "        :param dropout: Dropout probability.\n",
    "        \"\"\"\n",
    "        super(GCN3, self).__init__()\n",
    "        self.conv1 = GCNConv(num_feature, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        \"\"\" Perform a forward pass on the GCN3 model to get embeddings.\n",
    "        :param x: Input feature tensor of shape (num_nodes, num_features)\n",
    "        :param edge_index: Edge tensor of shape (2, num_edges)\n",
    "        :return: Output tensor of shape (num_nodes, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Two-layer GCN used for graph node encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param edge_index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder module that maps latent representation to output node features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 lstm_output_size: int = 32,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        \"\"\"\n",
    "        :param hidden_size: The size of the hidden layer in the Decoder.\n",
    "        :param output_size: The size of the output layer in the Decoder.\n",
    "        :param lstm_output_size: The output size of the LSTM layer in case of LSTM-based Decoder.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, output_size)\n",
    "        self.fc2 = nn.Linear(lstm_output_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(2, hidden_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Perform forward pass through the Decoder.\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # handle lstm\n",
    "        # print(f\" Forward {x.shape}, {self.fc2.in_features}\")\n",
    "        if x.shape[-1] == self.fc2.in_features:\n",
    "            print(\"############## LSTM case\")\n",
    "            return self.forward_lstm(x)\n",
    "        # handle gnn\n",
    "        elif x.shape[-1] == self.fc3.in_features:\n",
    "            return self.forward_generated(x)\n",
    "        else:\n",
    "            return self.forward_gcn(x)\n",
    "\n",
    "    def forward_gcn(self, x) -> Tensor:\n",
    "        \"\"\"Perform forward pass through the Decoder in case of a GNN-based Decoder.\n",
    "        :param x: The input tensor to the Decoder.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # print(\"gnc case\")\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward_lstm(self, x) -> Tensor:\n",
    "        \"\"\" Perform forward pass through the Decoder in case of a LSTM-based Decoder.\n",
    "        :param x: The input tensor to the Decoder.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward_generated(self, x):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        \"\"\"\n",
    "        :param input_size:\n",
    "        :param hidden_size:\n",
    "        :param num_layers:\n",
    "        \"\"\"\n",
    "        super(GraphLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, hidden_channels, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param num_features:\n",
    "        :param hidden_channels:\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features,\n",
    "                             hidden_channels,\n",
    "                             heads=8,\n",
    "                             dropout=dropout)\n",
    "        self.conv2 = GATConv(\n",
    "            hidden_channels * 8, hidden_channels,\n",
    "            dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x:\n",
    "        :param edge_index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphGenerationModel(Experiments):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 midi_dataset: MidiDataset,\n",
    "                 epochs: Optional[int] = 100,\n",
    "                 batch_size: Optional[int] = 32,\n",
    "                 embeddings_lr: Optional[float] = 0.01,\n",
    "                 lstm_lr: Optional[float] = 0.01,\n",
    "                 model_type: Optional[str] = \"GCN3\",\n",
    "                 gcn_hidden_dim: Optional[int] = 32):\n",
    "        \"\"\"\n",
    "        :param epochs:\n",
    "        :param batch_size:\n",
    "        :param midi_dataset:\n",
    "        \"\"\"\n",
    "        super().__init__(epochs, batch_size, midi_dataset)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        assert self.device is not None, \"Device is not set.\"\n",
    "        assert self.datasize is not None, \"Datasize is not set.\"\n",
    "        assert self.test_size is not None, \"Test size is not set.\"\n",
    "        assert self._num_workers is not None, \"Number of workers is not set.\"\n",
    "        assert self._batch_size is not None, \"Batch size is not set.\"\n",
    "\n",
    "        self.datasize = 0\n",
    "        self.test_size = 0\n",
    "        self._num_workers = 0\n",
    "        #\n",
    "        self.data_loader = DataLoader(\n",
    "            self._dataset, batch_size=self._batch_size, shuffle=True\n",
    "        )\n",
    "        self.model = None\n",
    "\n",
    "        self.hidden_channels = midi_dataset.num_classes\n",
    "        self.input_size = self.hidden_channels * 2\n",
    "        self.hidden_size = midi_dataset.num_classes * 2\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.num_layers = 1\n",
    "\n",
    "        print(f\"Dataset:            {midi_dataset}\")\n",
    "        print(f\"Epochs:             {epochs}\")\n",
    "        print(f\"Batch size:         {batch_size}\")\n",
    "        print(f\"Embeddings LR:      {embeddings_lr}\")\n",
    "        print(f\"LSTM LR:           {lstm_lr}\")\n",
    "        print(f\"Model type:         {model_type}\")\n",
    "        print(f\"GCN hidden dim:     {gcn_hidden_dim}\")\n",
    "        # print(f\"Feature dim:        {self._feature_dim}\")\n",
    "        # print(f\"Number of classes:  {self._num_classes}\")\n",
    "        # print(f\"Hidden dim:         {self._hidden_dim}\")\n",
    "\n",
    "        if model_type == \"GCN3\":\n",
    "            print(f\"Creating GCN3 {midi_dataset.num_features} {midi_dataset.num_classes}\")\n",
    "            self.gcn_model = GCN3(\n",
    "                midi_dataset.num_features, gcn_hidden_dim, midi_dataset.num_classes).to(self.device)\n",
    "        elif model_type == \"GAT\":\n",
    "            print(f\"Creating GAT {midi_dataset.num_features} {midi_dataset.num_classes}\")\n",
    "            self.gcn_model = GAT(\n",
    "                midi_dataset.num_features, midi_dataset.num_classes).to(self.device)\n",
    "            self._is_gat = True\n",
    "        else:\n",
    "            raise ValueError(\"unk\")\n",
    "            # self.model = GIN(\n",
    "            #     self._feature_dim, self._hidden_dim, self._num_classes)\n",
    "            # self._is_gin = True\n",
    "\n",
    "        # # self.gcn_model = GCN(midi_dataset.num_features, midi_dataset.num_classes).to(self.device)\n",
    "        # self.gcn_model = GAT(midi_dataset.num_features, midi_dataset.num_classes).to(self.device)\n",
    "\n",
    "        self.embeddings_lr: float = embeddings_lr\n",
    "        self.decoder = Decoder(midi_dataset.num_classes, midi_dataset.num_classes,\n",
    "                               lstm_output_size=self.hidden_size).to(self.device)\n",
    "        self.lstm_model = GraphLSTM(self.input_size, self.hidden_size, self.num_layers).to(self.device)\n",
    "        self.optimizer_gcn = torch.optim.Adam(self.gcn_model.parameters(), lr=self.embeddings_lr)\n",
    "        self.optimizer_lstm = torch.optim.Adam(self.lstm_model.parameters(), lr=lstm_lr)\n",
    "\n",
    "        # optimizer = torch.optim.Adam(list(gcn_model.parameters()) + list(gcn_encoder.parameters()) +\n",
    "        #                              list(decoder.parameters()) + list(graph_lstm.parameters()), lr=args.lr)\n",
    "\n",
    "    def train_gcn(self, grap_data: Data,\n",
    "                  epochs: int = 10,\n",
    "                  learning_rate: float = 0.01) -> tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        :param grap_data: A PyTorch Geometric `Data` object containing the graph data.\n",
    "        :param epochs: The number of epochs to train for.\n",
    "        :param learning_rate: The learning rate to use for optimization.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        gcn_ground_truth_labels = grap_data.y\n",
    "        correct_gcn_predictions = 0.0\n",
    "        total_gcn_predictions = 0.0\n",
    "        gcn_loss = 0.0\n",
    "\n",
    "        gcn_tp = np.zeros(self.num_classes)\n",
    "        gcn_fp = np.zeros(self.num_classes)\n",
    "        gcn_fn = np.zeros(self.num_classes)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            self.optimizer_gcn.zero_grad()\n",
    "            node_embeddings = self.gcn_model(grap_data.x, grap_data.edge_index)\n",
    "            output = self.decoder(node_embeddings)\n",
    "            loss_gcn = loss_fn(output, grap_data.y)\n",
    "            loss_gcn.backward()\n",
    "            self.optimizer_gcn.step()\n",
    "\n",
    "            gcn_loss += loss_gcn.item()\n",
    "            gcn_predicted_node_labels = torch.argmax(node_embeddings, dim=1)\n",
    "            correct_gcn_predictions += (gcn_predicted_node_labels == gcn_ground_truth_labels).sum().item()\n",
    "            total_gcn_predictions += gcn_ground_truth_labels.size(0)\n",
    "\n",
    "            for i in range(self.num_classes):\n",
    "                tp = ((gcn_predicted_node_labels == i) & (gcn_ground_truth_labels == i)).sum().item()\n",
    "                fp = ((gcn_predicted_node_labels == i) & (gcn_ground_truth_labels != i)).sum().item()\n",
    "                fn = ((gcn_predicted_node_labels != i) & (gcn_ground_truth_labels == i)).sum().item()\n",
    "                gcn_tp[i] += tp\n",
    "                gcn_fp[i] += fp\n",
    "                gcn_fn[i] += fn\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"epoch {epoch} gcn avg loss: {round(gcn_loss / (epoch + 1), 3)} \"\n",
    "                      f\"avg accuracy: {round(correct_gcn_predictions / total_gcn_predictions, 3)} \"\n",
    "                      f\"total gcn: {total_gcn_predictions}\")\n",
    "\n",
    "        gcn_precision = gcn_tp / (gcn_tp + gcn_fp + 1e-8)\n",
    "        gcn_recall = gcn_tp / (gcn_tp + gcn_fn + 1e-8)\n",
    "        gcn_f1_score = 2 * gcn_precision * gcn_recall / (gcn_precision + gcn_recall + 1e-8)\n",
    "\n",
    "        avg_gcn_loss = gcn_loss / epochs\n",
    "        avg_gcn_accuracy = correct_gcn_predictions / total_gcn_predictions\n",
    "\n",
    "        avg_gcn_precision = np.mean(gcn_precision)\n",
    "        avg_gcn_recall = np.mean(gcn_recall)\n",
    "        avg_gcn_f1_score = np.mean(gcn_f1_score)\n",
    "\n",
    "        print(f\"avg loss for 100 epoch: {avg_gcn_loss:.4f}, \"\n",
    "              f\"accuracy: {avg_gcn_accuracy:.4f}, \"\n",
    "              f\"precision: {avg_gcn_precision:.4f}, \"\n",
    "              f\"recall: {avg_gcn_recall:.4f}, \"\n",
    "              f\"F1-score: {avg_gcn_f1_score:.4f} \")\n",
    "        return gcn_loss, correct_gcn_predictions, total_gcn_predictions\n",
    "        # print(f\"Epoch {epoch}, Loss: {loss_gcn.item():.4f} shape {output.shape} {self._dataset.num_classes}\")\n",
    "\n",
    "    def train_lstm(\n",
    "            self, seq_data: List[torch.Tensor], epochs: int = 10, learning_rate: float = 0.01):\n",
    "        \"\"\"Train the LSTM model on a list of edge embeddings.\n",
    "\n",
    "        During training, we consider a true positive to be a correct prediction\n",
    "         of the next edge embedding in the sequence,\n",
    "         and a false positive to be an incorrect prediction.\n",
    "\n",
    "         The ground truth labels are the target edge embeddings that\n",
    "         come immediately after each input edge embedding in the sequence.\n",
    "\n",
    "         seq_data: A list of PyTorch tensors, where each tensor represents a sequence\n",
    "                    of edge and node embeddings. Each tensor has shape (1, seq_len, num_features),\n",
    "                    where seq_len is the length of the sequence and\n",
    "                     num_features is the number of features in each edge or node embedding.\n",
    "\n",
    "                     The tensor is constructed by concatenating the edge and\n",
    "                     node embeddings for each edge in the sequence.\n",
    "\n",
    "        :param seq_data: A list of edge embeddings, where each edge\n",
    "                          embedding has shape (num_edges, 2 * hidden_size).\n",
    "        :param epochs: The number of epochs to train the LSTM for.\n",
    "        :param learning_rate: The learning rate to use for optimization.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        correct_lstm_predictions = 0\n",
    "        total_lstm_predictions = 0\n",
    "        lstm_loss = 0\n",
    "\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            for seq in seq_data:\n",
    "                self.optimizer_lstm.zero_grad()\n",
    "                seq = seq.view(1, -1, self.input_size)\n",
    "                lstm_input = seq[:, :-1, :]\n",
    "                lstm_target = seq[:, 1:, :]\n",
    "                hidden = None\n",
    "                lstm_output, hidden = self.lstm_model(lstm_input, hidden)\n",
    "                lstm_ground_truth_labels = seq.clone()\n",
    "\n",
    "                loss_lstm = F.mse_loss(lstm_output, lstm_target)\n",
    "                loss_lstm.backward(retain_graph=True)\n",
    "                self.optimizer_lstm.step()\n",
    "                lstm_loss += loss_lstm.item()\n",
    "\n",
    "                lstm_predicted_labels = torch.argmax(lstm_output, dim=1)\n",
    "                correct_lstm_predictions += (lstm_predicted_labels == lstm_ground_truth_labels).sum().item()\n",
    "                total_lstm_predictions += lstm_ground_truth_labels.size(0)\n",
    "\n",
    "                # calculate precision, recall, and F1-score\n",
    "                true_positives += ((lstm_predicted_labels == lstm_ground_truth_labels)\n",
    "                                   & (lstm_ground_truth_labels == 1)).sum().item()\n",
    "                false_positives += ((lstm_predicted_labels != lstm_ground_truth_labels)\n",
    "                                    & (lstm_predicted_labels == 1)).sum().item()\n",
    "                false_negatives += ((lstm_predicted_labels != lstm_ground_truth_labels)\n",
    "                                    & (lstm_ground_truth_labels == 0)).sum().item()\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                avg_loss = lstm_loss / len(seq_data)\n",
    "                avg_accuracy = correct_lstm_predictions / total_lstm_predictions\n",
    "                precision = true_positives / (\n",
    "                        true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "                recall = true_positives / (\n",
    "                        true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "\n",
    "                f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "                print(f\"Epoch: {epoch + 1}, Avg LSTM loss: {avg_loss:.4f}, \"\n",
    "                      f\"Avg Accuracy: {avg_accuracy:.4f}, \"\n",
    "                      f\"Precision: {precision:.4f}, \"\n",
    "                      f\"Recall: {recall:.4f}, \"\n",
    "                      f\"F1-score: {f1_score:.4f}\")\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "        avg_lstm_loss = lstm_loss / len(seq_data)\n",
    "        avg_lstm_accuracy = correct_lstm_predictions / total_lstm_predictions\n",
    "\n",
    "        print(f\"LSTM avg loss {avg_lstm_loss:.4f} \"\n",
    "              f\"avg accuracy  {avg_lstm_accuracy:.4f} \"\n",
    "              f\"total prediction {total_lstm_predictions}, \"\n",
    "              f\"Recall {recall:.4f}, \"\n",
    "              f\"F1 {f1_score:.4f}\")\n",
    "\n",
    "        return lstm_loss, correct_lstm_predictions, total_lstm_predictions\n",
    "\n",
    "    def generate_new_nodes_edges(self, edge_embeddings: torch.Tensor, num_new_nodes: int = 1) -> torch.Tensor:\n",
    "        \"\"\" Generates new node embeddings and corresponding edges using the trained LSTM model.\n",
    "        :param edge_embeddings:  Edge embeddings of the test graph.\n",
    "        :param num_new_nodes: Number of new nodes to generate.\n",
    "        :return: Embeddings of the newly generated nodes and their corresponding edges.\n",
    "        \"\"\"\n",
    "        lstm_input = edge_embeddings.view(1, -1, self.input_size)\n",
    "        hidden = None\n",
    "        new_nodes_edges = []\n",
    "\n",
    "        for _ in range(num_new_nodes):\n",
    "            lstm_output, hidden = self.lstm_model(lstm_input, hidden)\n",
    "            lstm_output = lstm_output[-1, -1, :]\n",
    "            new_embedding = self.decoder(lstm_output)\n",
    "            new_nodes_edges.append(new_embedding.detach())\n",
    "\n",
    "        # convert the generated embeddings into hash values and edge weights\n",
    "        new_nodes_edges = torch.stack(new_nodes_edges)\n",
    "        return new_nodes_edges\n",
    "\n",
    "    def get_graph_edge_embeddings(self, _subgraph_data: Data) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param _subgraph_data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node_emb = self.gcn_model(_subgraph_data.x, _subgraph_data.edge_index)\n",
    "        node_emb = torch.cat([\n",
    "            node_emb[_subgraph_data.edge_index[0]],\n",
    "            node_emb[_subgraph_data.edge_index[1]]],\n",
    "            dim=1)\n",
    "        return node_emb\n",
    "\n",
    "\n",
    "def dijkstra_traversal2(graph, start_node):\n",
    "    \"\"\"Generates new node embeddings and\n",
    "       corresponding edges using the trained LSTM model.\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    pq = [(0, start_node)]\n",
    "    node_sequence = []\n",
    "    edge_sequence = []\n",
    "\n",
    "    edge_index = graph.edge_index.view(2, -1)\n",
    "\n",
    "    while pq:\n",
    "        dist, node = heapq.heappop(pq)\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            node_sequence.append(node)\n",
    "            neighbors = edge_index[1][edge_index[0] == node].tolist()\n",
    "            for neighbor in neighbors:\n",
    "                edge_sequence.append((node, neighbor))\n",
    "                if neighbor not in visited:\n",
    "                    edge_indices = (edge_index[0] == node) & (edge_index[1] == neighbor)\n",
    "                    if edge_indices.any():\n",
    "                        # use the first matching edge index\n",
    "                        edge_attr_index = edge_indices.nonzero(as_tuple=True)[0][0].item()\n",
    "                        edge_weight = graph.edge_attr[edge_attr_index]\n",
    "                        heapq.heappush(pq, (dist + edge_weight, neighbor))\n",
    "\n",
    "    return node_sequence, edge_sequence\n",
    "\n",
    "\n",
    "def compute_node_and_edge_embeddings(model, graph_data):\n",
    "    \"\"\"\n",
    "    :param model:\n",
    "    :param graph_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    node_embeddings = model.gcn_model(graph_data.x, graph_data.edge_index)\n",
    "    edge_embeddings = torch.cat([\n",
    "        node_embeddings[graph_data.edge_index[0]],\n",
    "        node_embeddings[graph_data.edge_index[1]]\n",
    "    ], dim=1)\n",
    "    return node_embeddings, edge_embeddings\n",
    "\n",
    "\n",
    "def map_labels_to_hashes(predicted_labels, hash_labels):\n",
    "    mapped_labels = [hash_labels[i] for i in predicted_labels.tolist()]\n",
    "    return mapped_labels\n",
    "\n",
    "\n",
    "def main(args, midi_dataset):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    sequence_data = []\n",
    "\n",
    "    graph_data_loader = DataLoader(\n",
    "        midi_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    graph_generation_model = GraphGenerationModel(\n",
    "        midi_dataset=midi_dataset,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        embeddings_lr=args.emb_lr,\n",
    "        lstm_lr=args.lstm_lr,\n",
    "        model_type=args.gcn,\n",
    "        gcn_hidden_dim=args.gcn_hidden_dim\n",
    "    )\n",
    "\n",
    "    for i, grap_data in enumerate(graph_data_loader):\n",
    "        node_sequence, edge_sequence = dijkstra_traversal2(grap_data, start_node=0)\n",
    "        graph_generation_model.train_gcn(grap_data, epochs=args.epochs)\n",
    "\n",
    "        # node embeddings using the trained GCN model for midi graph\n",
    "        node_embeddings = graph_generation_model.gcn_model(grap_data.x, grap_data.edge_index)\n",
    "        #  concatenate src and target node embeddings for each edge\n",
    "        edge_embeddings = torch.cat([node_embeddings[torch.tensor([src for src, _ in edge_sequence])],\n",
    "                                     node_embeddings[torch.tensor([dst for _, dst in edge_sequence])]], dim=1)\n",
    "        sequence_data.append(edge_embeddings.detach())\n",
    "\n",
    "    # train LSTM\n",
    "    # The node_embeddings tensor has the shape (3, 16)\n",
    "    # which means it has 3 rows (corresponding to the 3 nodes in the input graph)\n",
    "    # and 16 columns (representing the 16-dimensional embedding for each node).\n",
    "    graph_generation_model.train_lstm(sequence_data)\n",
    "    grap_data = grap_data.clone()\n",
    "\n",
    "    # generate new nodes and edges\n",
    "    new_node_embeddings = graph_generation_model.gcn_model(\n",
    "        grap_data.x, grap_data.edge_index)\n",
    "\n",
    "    edge_embeddings = torch.cat([node_embeddings[torch.tensor([src for src, _ in grap_data.edge_index.T])],\n",
    "                                 node_embeddings[torch.tensor([dst for _, dst in grap_data.edge_index.T])]], dim=1)\n",
    "\n",
    "    new_nodes_edges = graph_generation_model.generate_new_nodes_edges(edge_embeddings)\n",
    "    decoded_node = graph_generation_model.decoder(new_nodes_edges)\n",
    "\n",
    "    # new_nodes_edges = graph_generation_model.generate_new_nodes_edges(new_node_embeddings)\n",
    "    print(f\"node embeddings {new_node_embeddings.shape} {new_nodes_edges.shape}\")\n",
    "    predicted_labels = torch.argmax(decoded_node, dim=1)\n",
    "    print(f\"predicted_labels {predicted_labels}\")\n",
    "    mapped_labels = [dataset.hash_labels[i] for i in predicted_labels.tolist()]\n",
    "    print(f\"Mapped labels: {mapped_labels}\")\n",
    "    return decoded_node\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--hidden_dim', type=int, default=32)\n",
    "parser.add_argument('--emb_lr', type=float, default=0.01)\n",
    "parser.add_argument('--lstm_lr', type=float, default=0.01)\n",
    "parser.add_argument('--gcn_hidden_dim', type=int, default=32)\n",
    "parser.add_argument('--graph_per_instrument', type=bool, default=False)\n",
    "parser.add_argument('--random_split', type=bool, default=False)\n",
    "parser.add_argument('--gcn', type=str, default='GCN3', choices=['GCN', 'GAT'])\n",
    "parser.add_argument('--num_val', type=float, default=0.05)\n",
    "parser.add_argument('--num_test', type=float, default=0.1)\n",
    "parser.add_argument('--add_negative_train_samples', type=bool, default=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "])\n",
    "\n",
    "if args.random_split:\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device),\n",
    "        T.RandomLinkSplit(\n",
    "            num_val=args.num_val,\n",
    "            num_test=args.num_test,\n",
    "            is_undirected=True,\n",
    "            split_labels=True,\n",
    "            add_negative_train_samples=args.add_negative_train_samples)\n",
    "    ])\n",
    "else:\n",
    "    transform = T.Compose([\n",
    "        T.NormalizeFeatures(),\n",
    "        T.ToDevice(device),\n",
    "    ])\n",
    "\n",
    "dataset = MidiDataset(\n",
    "    root=\"./data\",\n",
    "    per_instrument_graph=args.graph_per_instrument,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(dataset.num_features)\n",
    "print(f\"Dataset size:       {len(dataset)}\")\n",
    "print(\"Number of classes\", dataset.total_num_classes)\n",
    "print(\"Number of feature\", dataset.num_features)\n",
    "print(f\"x shape:           {dataset[0].x.shape}\")\n",
    "print(f\"y shape:           {dataset[0].y.shape}\")\n",
    "print(f\"Label shape:       {dataset[0].label.shape}\")\n",
    "print(f\"number of classes: {dataset.num_classes}\")\n",
    "main(args, dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
